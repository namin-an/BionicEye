{"cells":[{"cell_type":"markdown","metadata":{"id":"MTJyfTn42mSg"},"source":["# **References**\n","\n","- https://colab.research.google.com/drive/1-KoHALGq7OcGtTYkUp2PPo90BMB3MR0x#scrollTo=OUkRDMXtzxI3"]},{"cell_type":"markdown","metadata":{"id":"zo0FcLBj1M1G"},"source":["# **The Problem**\n","\n","We want to train an artificial agent which gives human feedback information to the model and hopefully the model gives out higher facial recognition accuracy than when they were not fine-tuned."]},{"cell_type":"markdown","metadata":{"id":"YrdExhq78hjv"},"source":["We want to minimize the prediction error for each training step as much as possible."]},{"cell_type":"markdown","metadata":{"id":"PU5r7Sf82XqB"},"source":["Deep Q Network (DQN)\n","\n","$$\n","    E_{(s, a, r, s') - U(D)} (\\underbrace{r_{t}}_{\\rm current~reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{\\max_{a'}Q(s_{t+1}, a'; \\theta')}_{\\rm estimate~of~future~rewards} - \\underbrace{Q(s_{t},a_{t}; \\theta)}_{\\rm current~expectation})^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"HP2Gp5JsVRIT"},"source":["Double Deep Q Network (DDQN)\n","\n","$$\n","    E_{(s, a, r, s') - U(D)} (\\underbrace{r_{t}}_{\\rm current~reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{Q(s_{t+1}, argmax_a Q(s_{t+1}, a; \\theta); \\theta')}_{\\rm estimate~of~future~rewards} - \\underbrace{Q(s_{t},a_{t}; \\theta)}_{\\rm current~expectation})^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"6iTLJpmc2-Fa"},"source":["SARSA\n","\n","$$\n","    (\\underbrace{r_{t}}_{\\rm current~reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{Q(s_{t+1}, a_{t+1})}_{\\rm estimate~of~future~rewards} - \\underbrace{Q(s_{t},a_{t})}_{\\rm current~expectation})^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Mw85-2AYXMYE"},"source":["Expected SARSA\n","\n","$$\n","    (\\underbrace{r_{t}}_{\\rm current~reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{\\Sigma_a'\\pi(a'|s_{t+1})Q(s_{t+1}, a')}_{\\rm estimate~of~future~rewards} - \\underbrace{Q(s_{t},a_{t})}_{\\rm current~expectation})^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"fdY1eeU2KH3s"},"source":["(Advantage) Actor-Critic (A2C)\n","\n","$$\n","    - log\\pi(a_t|s_t; \\theta_\\pi) \\cdot (\\underbrace{r_{t}}_{\\rm current~reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{V(s_{t+1}; \\theta_v)}_{\\rm estimate~of~future~rewards} - \\underbrace{V(s_{t};\\theta_v)}_{\\rm current~expectation})\n","$$\n","\n","$$\n","    (\\underbrace{r_{t}}_{\\rm current~reward} + \\underbrace{\\gamma}_{\\rm discount} \\cdot \\underbrace{V(s_{t+1}; \\theta_v)}_{\\rm estimate~of~future~rewards} - \\underbrace{V(s_{t};\\theta_v)}_{\\rm current~expectation})^2\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["REINFORCE\n","\n","$$\n","    - log\\pi(a_t|s_t; \\theta_\\pi) \\cdot r_{t} + \\gamma \\cdot V(s_{t+1}; \\theta_v) + ... + \\gamma^T \\cdot V(s_{t+T}; \\theta_v)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"8sruUGE6XQmr"},"source":["Proximal Policy Optimization (PPO)\n","\n","$$\n","    maximize_\\theta \\ E[\\frac{\\pi_\\theta (a_t|s_t)}{\\pi_\\theta old (a_t|s_t)} \\cdot ( r_{t} + \\gamma \\cdot V(s_{t+1}; \\theta_v) - V(s_{t};\\theta_v) )]\n","$$\n","\n","$$\n","    subject\\ to \\ E[KL[\\pi_\\theta (\\cdot|s_t), \\pi_\\theta (\\cdot|s_t)]] < \\ \\delta\n","$$"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOjSQPGjOQ6pJzBV3yIoRD/","collapsed_sections":[],"mount_file_id":"1ykHS-abjpHAX5P2Wv_3bhrIk6ubgx6pk","provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('python39')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"d21f86f814c663a2839376199717de8ab40993b4cd7cf2a97226f6e7cf3338fb"}}},"nbformat":4,"nbformat_minor":0}
