{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":5,"status":"error","timestamp":1657783038177,"user":{"displayName":"Na Min An","userId":"07779782386374796484"},"user_tz":-540},"id":"6jfbgERM2Los","outputId":"fa9a6f9c-9861-4a36-cb9a-aea9ca208750"},"outputs":[],"source":["import datetime\n","\n","date = datetime.datetime.now()\n","print(f'Today is Happy{date: %A, %d, %m, %Y}.', '\\n')\n","\n","import os\n","from glob import glob\n","from itertools import combinations\n","import math\n","import random \n","from random import sample\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.lines import Line2D\n","import itertools\n","import scipy as sp\n","from scipy import stats\n","import scipy.stats as sp\n","from statannot import add_stat_annotation\n","import cv2 as cv\n","import torch\n","from torch.utils.data import DataLoader\n","\n","import sys\n","sys.path.append('../code')\n","from dataloader.kface16000 import KFaceDataLoader\n","from models import PPO, AC, DQN, CNN\n","\n","sns.set_context(\"paper\", rc={\"axes.titlesize\":16,\"axes.labelsize\":14, \"xtick.labelsize\":12, \"ytick.labelsize\":12})   \n","sns.set_style('whitegrid', rc={'font.family': 'serif', 'font.serif': 'Times New Roman'})"]},{"cell_type":"markdown","metadata":{"id":"8JJWkpvIkTVg"},"source":["### Button"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lE9TCvTkTVh"},"outputs":[],"source":["date = '2022-11-05'\n","time = '11-22-42'\n","windows = True # True for Windows, False for MAC\n","train = 'SL' # 'RL' or 'SL\n","stim_type = 'opt' # 'opt' or 'elec'\n","top1 = False # True for top1, False if top2\n","class_num = 16\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","if windows:\n","    base_dir = 'C:\\\\Users\\\\Na Min An\\\\Desktop\\\\Development\\\\Bioniceye\\\\code'\n","    slash = '\\\\'\n","else:\n","    base_dir = '/Users/naminan/Development/Bioniceye/code'\n","    slash = '/'\n","\n","data_dir = os.path.join(base_dir, 'data')\n","data_path = os.path.join(data_dir, '210827_ANNA_Removing_uncontaminated_data.csv')\n","label_path = os.path.join(data_dir, '211105_QAs_for_Set0_CNN_SVC_4classes_partial.csv')\n","pred_dir =  os.path.join(os.path.join(data_dir, 'Human_Expert'), '211202')\n","\n","df = pd.read_csv(data_path)\n","l = list(range(df.shape[0]))\n","random.seed(22)\n","set_1 = random.sample(l, class_num)\n","face_lst = [df.iloc[i, 0] for i in set_1]\n","face_dic_r = {int(i) : str(face) for (i, face) in enumerate(face_lst)}\n","\n","if stim_type == 'opt':\n","        image_dir = f'{data_dir}{slash}sample_for_dev_test'\n","elif stim_type == 'elec':\n","    image_dir = f'{data_dir}{slash}sample_for_dev_test2'\n","\n","if train == 'RL':\n","    env_type = 'Bioniceye' #'CartPole-v1'\n","    model_type = 'DQN' # 'PPO' or 'AC' or 'DQN'\n","    learning_rate, gamma, lmbda, eps_clip, batch_size = None, None, None, None, None\n","    outputs_dir = os.path.join(base_dir, 'outputs') # {slash}date{slash}time{slash}outputs\n","\n","    model_state_dict_files = glob(f'{outputs_dir}{slash}{date}{slash}{time}{slash}outputs{slash}{env_type}_{model_type}.pth', recursive=True)\n","    train_returns_files = glob(f'{outputs_dir}{slash}{date}{slash}{time}{slash}outputs{slash}TrainReturns_{env_type}_{model_type}.csv', recursive=True)\n","    correctness_files = glob(f'{outputs_dir}{slash}{date}{slash}{time}{slash}outputs{slash}Correctness_{env_type}_{model_type}.csv', recursive=True)\n","    model_state_dict_files = list(set(model_state_dict_files))\n","    model_state_dict_files.sort()\n","    train_returns_files.sort()\n","    correctness_files.sort()\n","    if model_type == 'DQN':\n","        model = DQN(16, 'Bioniceye', device)\n","    elif model_type == 'PPO':\n","        model = PPO(16, 'Bioniceye', device)\n","    assert len(model_state_dict_files) != 0\n","\n","elif train == 'SL':\n","    multirun_dir = os.path.join(base_dir, 'multirun')\n","    model_state_dict_files = glob(f'{multirun_dir}{slash}{date}{slash}**{slash}**{slash}outputs{slash}CNN.pth', recursive=True)\n","    model_state_dict_files = list(set(model_state_dict_files))\n","    model_state_dict_files.sort()\n","    model_state_dict_files = [model_state_dict_files[-1]] + model_state_dict_files[1:9]\n","    model = CNN(16)\n","    assert len(model_state_dict_files) == 9"]},{"cell_type":"markdown","metadata":{"id":"jiY1IV6WkTVi"},"source":["### Human data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"elapsed":765,"status":"ok","timestamp":1637308008612,"user":{"displayName":"Namin An","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcIdK97rsH9YA6C_FKaa02TWYZ4yul6A50fTSeCQ=s64","userId":"07779782386374796484"},"user_tz":-540},"id":"rZEU5y0R2co6","outputId":"4cef49e0-f69d-44bf-e81b-43d0cb72d967"},"outputs":[],"source":["if stim_type == 'opt':\n","  sel_ppl = list(range(300, 309)) + list(range(400, 408)) + [611] # 18 subjects\n","elif stim_type == 'elec': \n","  sel_ppl = [499, 500] + list(range(502, 509)) + list(range(601, 607)) + list(range(608, 611)) # 18 subjects\n","random.seed(42)\n","sel_ppl = random.sample(sel_ppl, len(sel_ppl)//2)\n","\n","human_df = pd.DataFrame()\n","preds = []\n","n = 9\n","for i in range(1, 80*n+1, 80):\n","    j = i+79 \n","    temp_df = pd.read_csv(os.path.join(pred_dir, f'main_test({i}_{j}).csv'))\n","    temp_df = temp_df[temp_df['유저식별아이디'].isin(sel_ppl)]\n","    human_df = pd.concat([human_df, temp_df], axis=1)\n","    # To calculate the mode\n","    if top1:\n","      temp_df = temp_df.loc[:, temp_df.columns.str.startswith('선택_A')]\n","      temp_df = temp_df.fillna(0)\n","      for k in range(len(temp_df.columns)):\n","          temp_preds = temp_df.iloc[:, k].values.astype(str).astype(float).astype(int)\n","          temp_preds = temp_preds[temp_preds > 0]\n","          temp_pred = np.bincount(temp_preds).argmax() \n","          preds.append(temp_pred)\n","    else:\n","      temp_df1 = temp_df.loc[:, temp_df.columns.str.startswith('선택_A')]\n","      temp_df2 = temp_df.loc[:, temp_df.columns.str.startswith('선택_B')]\n","      temp_df1, temp_df2 = temp_df1.fillna(0), temp_df2.fillna(0)\n","      for k in range(len(temp_df1.columns)):\n","          temp_preds1 = temp_df1.iloc[:, k].values.astype(str).astype(float).astype(int)\n","          temp_preds2 = temp_df2.iloc[:, k].values.astype(str).astype(float).astype(int)\n","          temp_preds = np.concatenate([temp_preds1, temp_preds2])\n","          temp_preds = temp_preds[temp_preds > 0]\n","          temp_pred = np.bincount(temp_preds).argmax() \n","          preds.append(temp_pred)  \n","        \n","orig_human_df = human_df\n","human_df = human_df.fillna(0)\n","\n","sel_col = []\n","for j in range(1, 80*n+1):\n","  if top1:\n","    temp_str = f'선택_A_{j}'\n","    sel_col.append(temp_str)\n","  else:\n","    temp_str1 = f'선택_A_{j}'\n","    temp_str2 = f'선택_B_{j}'\n","    sel_col.append(temp_str1)\n","    sel_col.append(temp_str2)\n","\n","human_df = human_df[sel_col]\n","human_df.index = sel_ppl \n","\n","if top1:\n","  human_df.columns = list(range(80*n))\n","else:\n","  human_df.columns = list(range(2*80*n))\n","\n","question_df = pd.read_csv(label_path)\n","\n","if top1:\n","  human_df.columns = question_df['Answer']\n","else:\n","  human_df.columns = [ans for ans in question_df['Answer'] for _ in (0, 1)]\n","\n","processed_human_df = human_df.copy()\n","for (j, ans_file) in enumerate(processed_human_df.columns):\n","    face, pix, gs, par = ans_file.split('.')[0].split('_')\n","    act_face = int(face)\n","    for i in range(processed_human_df.shape[0]):\n","        pred_face = int(processed_human_df.iloc[i, j])\n","        if act_face == pred_face:\n","            processed_human_df.iloc[i, j] = 1.0\n","        else:\n","            processed_human_df.iloc[i, j] = 0.0\n","\n","if not top1:\n","  processed_human_df = processed_human_df.iloc[:, 0:1440:2] + processed_human_df.iloc[:, 1:1440:2]\n","\n","processed_human_df_T = processed_human_df.transpose()\n","face_lst, pix_lst, gs_lst, par_lst, exp_lst = [], [], [], [], []\n","for (i, ans_file) in enumerate(processed_human_df_T.index):\n","  face, pix, gs, par = ans_file.split('.')[0].split('_')\n","  face_lst.append(face)\n","  pix_lst.append(pix)\n","  gs_lst.append(gs)\n","  par_lst.append(par)\n","  exp_lst.append(par.split('C')[0].split('E')[-1])\n","processed_human_df_T['Face'] = face_lst\n","processed_human_df_T['PIX'] = pix_lst\n","processed_human_df_T['GS'] = gs_lst\n","processed_human_df_T['Par'] = par_lst\n","processed_human_df_T['Exp'] = exp_lst\n","processed_human_df_T['Mean'] = processed_human_df_T[sel_ppl].mean(axis=1)\n","processed_human_df_T['Mode'] = preds\n","temp_lst = list(map(int, (processed_human_df_T['Mode'].astype(int) == processed_human_df_T['Face'].astype(int)).tolist()))\n","processed_human_df_T['Mode'] = temp_lst\n","\n","\n","setorder_lst = [n for n in range(9)]*80\n","setorder_lst.sort()\n","processed_human_df_T['Set'] = setorder_lst\n","\n","imp_par_lst = []\n","for i in range(processed_human_df_T.shape[0]):\n","  pix = processed_human_df_T['PIX'][i]\n","  gs = processed_human_df_T['GS'][i]\n","  exp = processed_human_df_T['Exp'][i]\n","  imp_par = f'{pix}_{gs}_{exp}' \n","  imp_par_lst.append(imp_par)\n","processed_human_df_T['ImpPar'] = imp_par_lst\n","\n","processed_human_df_T = processed_human_df_T.reset_index()\n","processed_human_df_T['Set'] = [idx//80+1 for idx in processed_human_df_T.index]\n","\n","processed_human_df_T"]},{"cell_type":"markdown","metadata":{},"source":["### Model Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hum_mod_df = processed_human_df_T.copy()\n","\n","if train == 'RL':\n","    for i in range(len(correctness_files)):\n","        temp_df = pd.DataFrame()\n","        correctness_df = pd.read_csv(correctness_files[i], index_col=0)\n","        correctness_df = correctness_df.dropna(axis=1, how='all')\n","        preepoch = 100\n","        correctness_df = correctness_df.iloc[:, preepoch:]\n","\n","        set_num = 9\n","        ques_num = correctness_df.shape[0] // set_num\n","        train_num = (correctness_df.shape[-1]-preepoch) // set_num \n","        assert (ques_num > 0) and (train_num > 0)\n","        ins_num = 9\n","        p = preepoch \n","        random.seed(77) \n","        for j in range(0, correctness_df.shape[0], ques_num):\n","            if j == 0:\n","                temp_df = correctness_df[[str(n) for n in sample(list(range(p, p+train_num)), ins_num)]].iloc[j:j+ques_num, :]\n","                temp_df.columns = range(ins_num)\n","            else:\n","                correctness_temp_df = correctness_df[[str(n) for n in sample(list(range(p, p+train_num)), ins_num)]].iloc[j:j+ques_num, :]\n","                correctness_temp_df.columns = range(ins_num)\n","                temp_df = pd.concat([temp_df, correctness_temp_df], axis=0, ignore_index=True)\n","            p += train_num\n","    temp_df.columns = [f'M{i}' for i in range(1, ins_num+1)]\n","    sel_mod = [f'M{i}' for i in range(1, ins_num+1)]\n","    hum_mod_df[sel_mod] = temp_df\n","    \n","elif train == 'SL':\n","    for i in range(len(model_state_dict_files)):\n","        KFaceDataset = KFaceDataLoader(image_dir, data_path, label_path, class_num, 'test', None)\n","        test_loader = DataLoader(KFaceDataset, batch_size=1, shuffle=False)\n","\n","        model = CNN(class_num).to(device)\n","        model.load_state_dict(torch.load(model_state_dict_files[i]))\n","\n","        temp_lst = []\n","        model.eval()\n","        with torch.no_grad():\n","            for (image, label) in test_loader:\n","                image, label = image.to(device), label.to(device)\n","                if train == 'RL':\n","                    image = torch.unsqueeze(image, 0)\n","                    if model_type == 'PPO' or model_type == 'AC':\n","                        pred_probs = model.forward_pi(image.float())\n","                    elif model_type == 'DQN':\n","                        pred_probs = model(image.float())\n","                elif train == 'SL':\n","                    pred_probs = model(image)\n","                _, pred_label = torch.max(pred_probs, dim=-1)\n","\n","                if int(pred_label) == int(label):\n","                    temp_lst.append(1.0)\n","                else:\n","                    temp_lst.append(0.0)\n","\n","        hum_mod_df[f'M{i+1}'] = temp_lst\n","\n","    sel_mod = [f'M{i}' for i in range(1, len(model_state_dict_files)+1)]\n","    hum_mod_df['Mean_SL'] = hum_mod_df[sel_mod].mean(axis=1) \n","\n","set_lst = [n+1 for n in range(9)] * 80\n","set_lst.sort()\n","hum_mod_df['Set'] = set_lst\n","\n","hum_mod_df"]},{"cell_type":"markdown","metadata":{},"source":["### Graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwD6ccbHkTVk","outputId":"1a1621da-e1f4-4dc0-e0c1-f69f1053edce"},"outputs":[],"source":["# Check nan values (x-axis: ID #, 0 means there was no answer)\n","bin_arr = np.where(human_df.values > 0.0, 1.0, 0.0).flatten()\n","sns.histplot(bin_arr, color=sns.color_palette('hls', 8)[0])\n","plt.ticklabel_format(axis='y', style='sci', scilimits=(0, 4))\n","plt.xticks(range(0, 2))\n","plt.xlabel('No answer (0) vs. filled answer (1)')\n","plt.title('Missing value distribution', y=1.03)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check age density \n","sns.histplot(data=orig_human_df['나이'], stat='density', kde=True)\n","plt.yticks(np.arange(0.0, 0.21, 0.05))\n","plt.xlabel('Age')\n","plt.title('Age distribution of human participants', y=1.03)\n","plt.legend([])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw accuracy values per set order\n","x = range(processed_human_df.shape[-1])\n","y_mean, y_stderr = processed_human_df.mean(axis=0), processed_human_df.std(axis=0) / math.sqrt(processed_human_df.shape[0])\n","sns.lineplot(x=x, y=y_mean, color=sns.color_palette('hls', 8)[1])\n","plt.fill_between(x=x, y1=y_mean - y_stderr, y2=y_mean + y_stderr, color=sns.color_palette('hls', 8)[1], alpha=0.5)\n","plt.xlabel('Question order')\n","plt.ylabel('Facial recognition accuracy')\n","plt.title('Accuracy change over the human experiment', y=1.03)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw accuracy values per set order (with hues)\n","hue_order = ['16PIX_2GS_03', '16PIX_2GS_02', '16PIX_2GS_01',\n","             '16PIX_4GS_03', '16PIX_4GS_02', '16PIX_4GS_01',\n","             '16PIX_8GS_03', '16PIX_8GS_02', '16PIX_8GS_01',\n","             '32PIX_2GS_03', '32PIX_2GS_02', '32PIX_2GS_01',\n","             '32PIX_4GS_03', '32PIX_4GS_02', '32PIX_4GS_01',\n","             '32PIX_8GS_03', '32PIX_8GS_02', '32PIX_8GS_01',\n","             '64PIX_2GS_03', '64PIX_2GS_02', '64PIX_2GS_01',\n","             '64PIX_4GS_03', '64PIX_4GS_02', '64PIX_4GS_01',\n","             '64PIX_8GS_03', '64PIX_8GS_02', '64PIX_8GS_01']\n","\n","plt.figure(figsize=(30, 6.5))\n","sns.boxplot(x='Set', y='Mean', hue='ImpPar', data=processed_human_df_T, palette=sns.color_palette(\"hls\", 27),\n","            hue_order=hue_order)\n","plt.xlabel('Set order')\n","plt.ylabel('Facial recognition accuracy')\n","plt.title('Accuracy change over the human experiment', y=1)\n","plt.legend(bbox_to_anchor=(1, 1.02))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation of performances in image-level\n","sel_df = hum_mod_df[sel_ppl + sel_mod]\n","sel_df = sel_df.astype(float)\n","corr = sel_df.corr()\n","corr.index = [f'H{i}' for i in range(1, len(sel_ppl)+1)] + sel_mod\n","corr.columns = [f'H{i}' for i in range(1, len(sel_ppl)+1)] + sel_mod\n","\n","sns.set_context(\"paper\", rc={\"axes.titlesize\":12,\"axes.labelsize\":9, \"xtick.labelsize\":7, \"ytick.labelsize\":7})   \n","\n","sns.heatmap(corr, vmin=0.0, vmax=1.0, cmap=sns.color_palette('Spectral_r', as_cmap=True))\n","plt.title('Correlation of Performances in Image-Level')\n","plt.xlabel('Human subjects (H1-H18) and Model variations (M1-M18)')\n","plt.ylabel('Human subjects (H1-H18) and Model variations (M1-M18)')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["choose = sel_mod # sel_ppl or sel_mod\n","choose_title = 'Artificial Agents' # 'Human Subjects' or 'SL Models' or 'Artificial Agents'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw average test performance over trials\n","sns.set_context(\"paper\", rc={\"axes.titlesize\":16,\"axes.labelsize\":14, \"xtick.labelsize\":12, \"ytick.labelsize\":12})   \n","\n","pix_lst = ['16PIX', '32PIX', '64PIX']\n","hum_mod_df_temp = hum_mod_df.copy()\n","hum_mod_df_temp.index = hum_mod_df['Set']\n","hum_mod_df_temp = hum_mod_df_temp[choose].stack().reset_index()\n","hum_mod_df_temp.columns = ['Set', 'Model', 'Mean']\n","plt.figure(figsize=(8, 8))\n","sns.lineplot(x='Set', y='Mean', data=hum_mod_df_temp, color=sns.color_palette('hls', len(pix_lst)+1)[3], linestyle='--') #sns.color_palette('hls', len(pix_lst)+1)[3], linestyle='--') # #d7bdf1 (light purple) # #7b06d9 (dark purple)\n","plt.xlim([0.3, 9.7])\n","plt.xticks(range(1, 10))\n","plt.ylim([0.4, 1.0])\n","plt.xlabel('Set Order')\n","plt.ylabel('Facial Recognition Accuracy')\n","plt.gca().set_yticklabels([f'{x:.0%}' for x in plt.gca().get_yticks()]) \n","plt.show()\n","\n","# Save the dataframes for the correlation coefficient calculation\n","mean_std_df = hum_mod_df_temp.groupby('Set').agg(['mean', 'std'])\n","mean_std_df.to_csv(f'MeanStd_{choose_title}.csv')\n","hum_mod_df_temp.to_csv(f'Raw_{choose_title}.csv')\n","\n","# Perform Mann-Whitney U test\n","for (i, j) in list(combinations(range(1, 10), 2)):\n","    if (i < j):\n","        U1, p = stats.ttest_ind(hum_mod_df_temp[hum_mod_df_temp['Set']==i]['Mean'], hum_mod_df_temp[hum_mod_df_temp['Set']==j]['Mean'],\n","                                alternative='less')\n","        if (p > 0.05):\n","            pass\n","        elif (0.01 < p <= 0.05):\n","            print(f'Set {i} & Set {j}: * (p={p:.2f})')\n","        elif (0.001 < p <= 0.01):\n","            print(f'Set {i} & Set {j}: ** (p={p:.3f})')\n","        else:\n","            print(f'Set {i} & Set {j}: *** (p={p:.7f})')\n","\n","# Calculate the correlation of each human subject and artificial agent (n = 729, AVERAGE)\n","set_df = hum_mod_df.groupby('Set').mean()\n","\n","x_arr, y_arr = np.array([]), np.array([])\n","for mod in sel_mod:\n","    x, y = set_df['Mean'].values, set_df[mod].values\n","    x_arr = np.append(x_arr, x)\n","    y_arr = np.append(y_arr, y)\n","\n","res = stats.pearsonr(x_arr, y_arr)\n","print('(R, p) =', res, ', n =', len(x_arr))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw test performance over trials for each number of pixels\n","sns.set_context(\"paper\", rc={\"axes.titlesize\":16,\"axes.labelsize\":14, \"xtick.labelsize\":12, \"ytick.labelsize\":12}) \n","\n","pix_lst = ['16PIX', '32PIX', '64PIX']\n","for (i, pix) in enumerate(pix_lst):\n","    pix_df_temp = hum_mod_df[hum_mod_df['PIX'] == pix].reset_index()\n","\n","    # Calculate the correlation of each human subject and artificial agent (n = 729, AVERAGE)\n","    set_df = pix_df_temp.groupby('Set').mean()\n","\n","    x_arr, y_arr = np.array([]), np.array([])\n","    x = set_df['Mean'].values\n","    for mod in sel_mod:\n","        y = set_df[mod].values\n","        x_arr = np.append(x_arr, x)\n","        y_arr = np.append(y_arr, y)\n","\n","    res = stats.pearsonr(x_arr, y_arr)\n","    print('(R, p) =', res, ', n =', len(x_arr))\n","\n","    pix_df = pix_df_temp[choose].stack().reset_index() \n","    pix_df.columns = ['Trial', 'Model', 'Mean']\n","\n","    # Performances in set-level\n","    pix_df_temp.index = pix_df_temp['Set']\n","    pix_df = pix_df_temp[choose].stack().reset_index() \n","    pix_df.columns = ['Set', 'Model', 'Mean']\n","    plt.figure(figsize=(8, 4))\n","    sns.lineplot(x='Set', y='Mean', data=pix_df, color=sns.color_palette('rocket_r', len(pix_lst)+1)[i], linestyle='--')\n","    plt.ylim([0.2, 1.0])\n","    plt.xlabel('Set Order')\n","    plt.ylabel('Facial Recognition Accuracy')\n","    plt.gca().set_yticklabels([f'{x:.0%}' for x in plt.gca().get_yticks()]) \n","    plt.title(f'{choose_title}')\n","    plt.show()\n","\n","    # Perform Mann-Whitney U test\n","    for (i, j) in list(combinations(range(1, 10), 2)):\n","        if (i < j):\n","            x = pix_df[pix_df['Set']==i]['Mean']\n","            y = pix_df[pix_df['Set']==j]['Mean']\n","            U1, p = stats.ttest_ind(x, y, alternative='less')\n","            if (p > 0.05):\n","                pass\n","            elif (0.01 < p <= 0.05):\n","                print(f'Set {i} (n={x.shape[0]}) & Set {j} (n={y.shape[0]}): *')\n","            elif (0.001 < p <= 0.01):\n","                print(f'Set {i} (n={x.shape[0]}) & Set {j} (n={y.shape[0]}): **')\n","            else:\n","                print(f'Set {i} (n={x.shape[0]}) & Set {j} (n={y.shape[0]}): ***')\n","    \n","    # Save the dataframes for the correlation coefficient calculation\n","    mean_std_df = pix_df.groupby('Set').agg(['mean', 'std'])\n","    mean_std_df.to_csv(f'MeanStd_{choose_title}_{pix}.csv')\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw accuracy values per set order\n","plt.figure(figsize=(30, 1))\n","x = range(processed_human_df_T.shape[0])\n","sns.scatterplot(x=x, y='Mode', data=processed_human_df_T, color=sns.color_palette('hls', 8)[2])\n","plt.xlim([0, 720])\n","plt.xlabel('Trial order')\n","plt.ylabel('Correctness')\n","plt.title('Facial recognition accuracy change over the human experiment', y=1.03)\n","plt.show()\n","\n","for i in range(2):\n","    count = pd.Series.value_counts(processed_human_df_T['Mode'])[i]\n","    tot_count = pd.Series.count(processed_human_df_T['Mode'])\n","    ratio = count / tot_count\n","    print(f\"Ratio of {i}'s: {ratio : .2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw accuracy plots based on PIX & GS\n","processed_human_df_T = processed_human_df_T.sort_values(by=['PIX', 'GS'])\n","\n","sns.lineplot(x='PIX', y='Mean', data=processed_human_df_T, hue='GS', err_style='band', ci=90, palette=sns.color_palette('flare', 3), legend=True, marker='o', markersize=7)\n","plt.ylim([0.2, 1])\n","plt.xticks(np.arange(3), labels=['16 PIX', '32 PIX', '64 PIX'])\n","plt.yticks(np.arange(0.2, 1.1, 0.2))\n","plt.xlabel('The number of pixels')\n","plt.ylabel('Facial recognition accuracy', labelpad=10)\n","plt.title('Behavioral task performance', y=1.03)\n","plt.legend(title='Grayscales', labels=['2 GS', '4 GS', '8 GS'])\n","plt.show()\n","\n","sns.lineplot(x='PIX', y='Mode', data=processed_human_df_T, hue='GS', err_style='band', ci=90, palette=sns.color_palette('flare', 3), legend=True, marker='o', markersize=7)\n","plt.ylim([0, 1])\n","plt.xticks(np.arange(3), labels=['16 PIX', '32 PIX', '64 PIX'])\n","plt.yticks(np.arange(0.0, 1.1, 0.2))\n","plt.xlabel('The number of pixels')\n","plt.ylabel('Facial recognition accuracy', labelpad=10)\n","plt.title('Behavioral task performance', y=1.03)\n","plt.legend(title='Grayscales', labels=['2 GS', '4 GS', '8 GS'])\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw accuracy plots based on pars\n","processed_human_df_T = processed_human_df_T.sort_values(by='Par')\n","hue_order = [('16PIX', '2GS'), ('16PIX', '4GS'), ('16PIX', '8GS'),\n","             ('32PIX', '2GS'), ('32PIX', '4GS'), ('32PIX', '8GS'),\n","             ('64PIX', '2GS'), ('64PIX', '4GS'), ('64PIX', '8GS')]\n","\n","sns.lineplot(x='Par', y='Mean', data=processed_human_df_T, err_style='band', ci=90, palette=sns.color_palette('hls', 9), marker='o', markersize=7,\n","             hue=processed_human_df_T[['PIX', 'GS']].apply(tuple, axis=1),\n","             hue_order=hue_order)\n","plt.xticks(rotation=30, y=-0.1)\n","plt.yticks(np.arange(0.2, 1.1, 0.2))\n","plt.xlabel('Facial parameters')\n","plt.ylabel('Facial recognition accuracy', labelpad=10)\n","plt.title('Behavioral task performance', y=1.03)\n","plt.legend(title='PIQ levels', bbox_to_anchor=(1, 1.02),\n","           labels=['16 PIX & 2 GS', '16 PIX & 4 GS', '16 PIX & 8 GS',\n","                   '32 PIX & 2 GS', '32 PIX & 4 GS', '32 PIX & 8 GS',\n","                   '64 PIX & 2 GS', '64 PIX & 4 GS', '64 PIX & 8 GS'])\n","plt.show()\n","\n","sns.lineplot(x='Par', y='Mode', data=processed_human_df_T, err_style='band', ci=90, palette=sns.color_palette('hls', 9), marker='o', markersize=7,\n","             hue=processed_human_df_T[['PIX', 'GS']].apply(tuple, axis=1),\n","             hue_order=hue_order)\n","plt.xticks(rotation=30, y=-0.1)\n","plt.yticks(np.arange(0.0, 1.1, 0.2))\n","plt.xlabel('Facial parameters', y=1.5)\n","plt.ylabel('Facial recognition accuracy', labelpad=10)\n","plt.title('Behavioral task performance', y=1.03)\n","plt.legend(title='PIQ levels', bbox_to_anchor=(1, 1.02),\n","           labels=['16 PIX & 2 GS', '16 PIX & 4 GS', '16 PIX & 8 GS',\n","                   '32 PIX & 2 GS', '32 PIX & 4 GS', '32 PIX & 8 GS',\n","                   '64 PIX & 2 GS', '64 PIX & 4 GS', '64 PIX & 8 GS'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check p-values for the averaged pars\n","box_pairs = [('S001L1E01C10', 'S001L1E01C4'),\n","             ('S001L1E01C10', 'S001L1E01C7'),\n","             ('S001L1E01C10', 'S001L1E02C7'),\n","             ('S001L1E01C10', 'S001L1E03C7'),\n","             ('S001L1E01C4', 'S001L1E01C7'),\n","             ('S001L1E01C4', 'S001L1E02C7'),\n","             ('S001L1E01C4', 'S001L1E03C7'),\n","             ('S001L1E01C7', 'S001L1E02C7'),\n","             ('S001L1E01C7', 'S001L1E03C7'),\n","             ('S001L1E02C7', 'S001L1E03C7')]\n","\n","plot = sns.lineplot(x='Par', y='Mean', data=processed_human_df_T, err_style='band', ci=90, palette=sns.color_palette('hls', 9), marker='o', markersize=7)\n","plot, test_results = add_stat_annotation(plot, data=processed_human_df_T, x='Par', y='Mean',\n","                                         box_pairs=box_pairs, test='Mann-Whitney', text_format='star', loc='outside', verbose=1)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Draw accuracy plots based on faces\n","processed_human_df_T = processed_human_df_T.sort_values(by='Face')\n","hue_order = [('16PIX', '2GS'), ('16PIX', '4GS'), ('16PIX', '8GS'),\n","             ('32PIX', '2GS'), ('32PIX', '4GS'), ('32PIX', '8GS'),\n","             ('64PIX', '2GS'), ('64PIX', '4GS'), ('64PIX', '8GS')]\n","\n","sns.lineplot(x='Face', y='Mean', data=processed_human_df_T, err_style='band', ci=90, palette=sns.color_palette('hls', 9), marker='o', markersize=7,\n","             hue=processed_human_df_T[['PIX', 'GS']].apply(tuple, axis=1),\n","             hue_order=hue_order)\n","plt.xticks(rotation=30, y=-0.1)\n","plt.yticks(np.arange(0.2, 1.1, 0.2))\n","plt.xlabel('Human facial IDs')\n","plt.ylabel('Facial recognition accuracy', labelpad=10)\n","plt.title('Behavioral task performance', y=1.03)\n","plt.legend(title='PIQ levels', bbox_to_anchor=(1, 1.02),\n","           labels=['16 PIX & 2 GS', '16 PIX & 4 GS', '16 PIX & 8 GS',\n","                   '32 PIX & 2 GS', '32 PIX & 4 GS', '32 PIX & 8 GS',\n","                   '64 PIX & 2 GS', '64 PIX & 4 GS', '64 PIX & 8 GS'])\n","plt.show()\n","\n","sns.lineplot(x='Face', y='Mode', data=processed_human_df_T, err_style='band', ci=90, palette=sns.color_palette('hls', 9), marker='o', markersize=7,\n","             hue=processed_human_df_T[['PIX', 'GS']].apply(tuple, axis=1),\n","             hue_order=hue_order)\n","plt.xticks(rotation=30, y=-0.1)\n","plt.yticks(np.arange(0.0, 1.1, 0.2))\n","plt.xlabel('Human facial IDs', y=1.5)\n","plt.ylabel('Facial recognition accuracy', labelpad=10)\n","plt.title('Behavioral task performance', y=1.03)\n","plt.legend(title='PIQ levels', bbox_to_anchor=(1, 1.02),\n","           labels=['16 PIX & 2 GS', '16 PIX & 4 GS', '16 PIX & 8 GS',\n","                   '32 PIX & 2 GS', '32 PIX & 4 GS', '32 PIX & 8 GS',\n","                   '64 PIX & 2 GS', '64 PIX & 4 GS', '64 PIX & 8 GS'])\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Training curves"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check average train returns\n","train_returns_df = pd.DataFrame()\n","for (i, file_path) in enumerate(train_returns_files):\n","    train_returns_df_temp = pd.read_csv(file_path, index_col=0)\n","    train_returns_df_temp = train_returns_df_temp.transpose().stack().to_frame().reset_index()\n","    train_returns_df_temp.columns = ['Epochs', 'Timestamps', 'Reward']\n","    train_returns_df_temp['Models'] = [file_path.split('.')[0].split(f'{slash}')[-1].split('_')[-1]] * train_returns_df_temp.shape[0]\n","    if i == 0:\n","        train_returns_df = train_returns_df_temp\n","    else:\n","        train_returns_df = pd.concat([train_returns_df, train_returns_df_temp], axis=0)\n","train_returns_df = train_returns_df.reset_index()\n","train_returns_df['Epochs'] = train_returns_df['Epochs'].astype(int)\n","\n","model_order = ['DQN', 'AC', 'PPO', 'REINFORCE']\n","if env_type == 'CartPole-v1':\n","    train_returns_df = train_returns_df.groupby(['Epochs', 'Models']).agg('sum').reset_index()\n","\n","sns.lineplot(x='Epochs', y='Reward', data=train_returns_df, err_style='band', palette='husl', hue='Models', hue_order=model_order)\n","\n","plt.xticks(range(0, train_returns_df['Epochs'].max() + 2, (train_returns_df['Epochs'].max() + 1) // 5))\n","plt.xlabel('Training episodes')\n","plt.ylabel('Average reward per episode')\n","\n","if env_type == 'CartPole-v1':\n","    plt.ylim([0, 510])\n","    plt.legend(loc='lower right')\n","plt.title('Training curves', y=1.03)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for (i, model_file) in enumerate(model_state_dict_files):\n","    model_name = model_file.split(f'{slash}')[-1].split('.')[0].split('_')[-1]\n","    model_name = 'PPO'\n","    temp_df = train_returns_df[train_returns_df['Models']==model_name]\n","\n","    sns.lineplot(x='Epochs', y='Reward', data=temp_df, err_style='band', color=sns.color_palette('hls', 6)[5]) \n","    plt.xticks(range(0, train_returns_df['Epochs'].max() + 2, (train_returns_df['Epochs'].max() + 1) // 5))\n","    plt.xlabel('Training Episodes')\n","    plt.ylabel(f'Average Reward')\n","    if env_type == 'CartPole-v1':\n","        plt.ylim([0, 510])\n","        plt.legend(loc='lower right')\n","    else:\n","        plt.ylim([0, 1])\n","        plt.ticklabel_format(style='sci', axis='x', scilimits=(0,1))\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Model weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# visualize the weights for the best saved model\n","img_num_row = 8\n","for file in model_state_dict_files:\n","    print(file)\n","    model_dict = torch.load(file)\n","    for (i, layer_name) in enumerate(model_dict):\n","        layer_weights = model_dict[layer_name].cpu()\n","        print(layer_name, layer_weights.shape)\n","\n","        if layer_name.split('.')[-1] == 'weight':\n","            if layer_name.split('.')[0] == 'cnn_num_block':\n","                fig, axes = plt.subplots(layer_weights.shape[0]//img_num_row, img_num_row, figsize=(img_num_row, layer_weights.shape[0]//img_num_row))\n","                for j in range(layer_weights.shape[0]):\n","                    axes[j//img_num_row, j%img_num_row].imshow(layer_weights[j, 0, :, :])\n","                    axes[j//img_num_row, j%img_num_row].set_xticklabels([])\n","                    axes[j//img_num_row, j%img_num_row].set_yticklabels([])\n","                    axes[j//img_num_row, j%img_num_row].axis('off')\n","            elif layer_name.split('.')[0] == 'linear_num_block_pi':\n","                layer_weights = layer_weights.mean(axis=-1)\n","                plt.plot(layer_weights)\n","                plt.xlabel('Artificial Neuron #')\n","                plt.ylabel('Learned Activation Value')\n","            plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load image\n","img = cv.imread('C:\\\\Users\\\\Na Min An\\\\Desktop\\\\Development\\\\Project\\\\code\\\\data\\\\sample_for_dev_test\\\\19090222\\\\32PIX_4GS_S001L1E01C7.jpg', cv.IMREAD_GRAYSCALE)\n","img = np.array(img.astype(np.float32))\n","plt.imshow(img, cmap='gray')\n","plt.axis('off')\n","plt.show()\n","\n","img = torch.Tensor(img)\n","img = torch.unsqueeze(img, axis=0)\n","if train == 'RL':\n","    img = img.to(device)\n","print(img.shape)\n","\n","\n","# Load model\n","model.load_state_dict(torch.load(model_state_dict_files[0]))\n","print(model)\n","\n","\n","# Concatenate info about layers\n","model_weights, layers = [], []\n","features = model.cnn_num_block\n","if train == 'RL':\n","    linears = model.linear_num_block_pi\n","elif train == 'SL':\n","    linears = model.linear_num_block\n","\n","for i in range(len(features)):\n","    if type(features[i]) != torch.nn.modules.activation.ReLU:\n","        layers.append(features[i])\n","        if type(features[i]) == torch.nn.Conv2d:\n","            model_weights.append(features[i].weight)\n","\n","for i in range(len(linears)):\n","    if type(features[i]) != torch.nn.modules.activation.ReLU:\n","        model_weights.append(linears[i].weight)\n","        layers.append(linears[i])\n","\n","print(\"Total # of conv and linear layers: \", len(model_weights))\n","print(\"Total # of layers except for ReLU: \", len(layers))\n","\n","\n","# Propagate the image into each layer of the model\n","ReLU = torch.nn.ReLU()\n","out = [layers[0](img)]\n","print(out[0].shape)\n","\n","for i in range(1, len(layers)):\n","    try:\n","        print(layers[i](out[-1]).shape)\n","        if type(layers[i]) == torch.nn.Conv2d:\n","            out.append(layers[i](out[-1]))\n","        else:\n","            out.append(layers[i](ReLU(out[-1])))\n","    except:\n","        print(layers[i](torch.flatten(out[-1])).shape)\n","        if i != len(layers)-1:\n","            out.append(layers[i](torch.flatten(out[-1])))\n","        else:\n","            out.append(layers[i](ReLU(out[-1])))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img_num_row = 16\n","for layer_num in range(len(out)):\n","    layer_vis = out[layer_num].data\n","    filt_num = out[layer_num].shape[0]\n","    \n","    if len(out[layer_num].size()) > 1:\n","        plt.figure(figsize=(img_num_row, filt_num//img_num_row))\n","        for (i, filter) in enumerate(layer_vis):\n","            filter = filter.cpu().numpy()\n","            plt.subplot(filt_num//img_num_row, img_num_row, i+1)\n","            plt.imshow(filter, cmap='gray')\n","            plt.axis('off')\n","    else:\n","        plt.plot(layer_vis.cpu().numpy())\n","    plt.tight_layout(pad=0.0)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"220208_ANNA_Fig3-1_FB_5.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.13 ('python39')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"d21f86f814c663a2839376199717de8ab40993b4cd7cf2a97226f6e7cf3338fb"}}},"nbformat":4,"nbformat_minor":0}
